{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2764a755-0131-413e-9782-a8566a0a8fe1",
   "metadata": {},
   "source": [
    "Task 1: Tabular Q-Learning Update\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a6b5d033-7db5-4560-b4cb-b6f42bc6d997",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m  \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mrandom\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'numpy'"
     ]
    }
   ],
   "source": [
    "import numpy as  np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def init_q_table(n_states, n_actions):\n",
    "    q_table = np.zeros((n_states, n_actions))\n",
    "    return q_table\n",
    "\n",
    "#print(init_q_table(2,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c264ebd9-5193-4843-815a-7c4208e49f02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_update(Q,s,a,r,s_next,alpha,gamma):\n",
    "    current_q = Q[s,a]\n",
    "    a_ = np.argmax(Q[s_next])\n",
    "    max_future_q = Q[s_next,a_]\n",
    "    \n",
    "    new_current_q = current_q + alpha*(r + gamma*max_future_q - current_q)\n",
    "    \n",
    "    Q[s,a] = new_current_q\n",
    "    \n",
    "    return Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea990158-e6e8-4396-b9af-a9390897031e",
   "metadata": {},
   "source": [
    "Task 2: ε-Greedy Policy on a Custom GridWorld\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d86a2cec-bb9f-4adf-9cdd-1e09c1439cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class gridWorld:\n",
    "    def __init__(self, size=4):\n",
    "        self.size = size\n",
    "        self.start_state = (0,0)\n",
    "        self.terminal_state = (3,3)\n",
    "        self.state = self.start_state\n",
    "        \n",
    "        self.actions = {0 : (0,1), 1 : (0,-1), 2 : (-1,0), 3 : (1,0)}\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = self.start_state\n",
    "        return self.state\n",
    "    \n",
    "    def step(self, action):\n",
    "        move = self.actions[action]\n",
    "        new_state = (self.state[0] + move[0], self.state[1] + move[1])\n",
    "        \n",
    "        new_state = (\n",
    "            max(0, min(self.size-1, self.state[0])),\n",
    "            max(0, min(self.size-1, self.state[1]))\n",
    "        )\n",
    "        \n",
    "        self.state = new_state\n",
    "        \n",
    "        if self.state == self.terminal_state:\n",
    "            return self.state, 1.0, True\n",
    "        else:\n",
    "            return self.state, 0.0, False\n",
    "        \n",
    "    def get_state_nos(self):\n",
    "        return self.state[0]*self.size + self.state[1]\n",
    "    \n",
    "    def get_states(self):\n",
    "        return [(i, j) for i in range(self.size) for j in range(self.size)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5ad85c33-8873-416c-b365-78bb2854fd7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_action(Q, state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        action = random.randint(0,3)\n",
    "    else:\n",
    "        action = np.argmax(Q[state])\n",
    "        \n",
    "    return action  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "32ea2ab2-3ee3-41d6-b6c1-8bc154df4aa4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'init_q_table' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m result \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epsilon \u001b[38;5;129;01min\u001b[39;00m epsilons:\n\u001b[1;32m---> 18\u001b[0m     Q \u001b[38;5;241m=\u001b[39m \u001b[43minit_q_table\u001b[49m(n_state, n_actions)\n\u001b[0;32m     19\u001b[0m     total_rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     20\u001b[0m     avg_rewards \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mNameError\u001b[0m: name 'init_q_table' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# traning in the gridWorld  \n",
    "\n",
    "env = gridWorld()\n",
    "\n",
    "episodes = 500\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "\n",
    "n_state = len(env.get_states())\n",
    "#print(len(n_state)) \n",
    "n_actions = len(env.actions)\n",
    "#print(len(n_actions))\n",
    "\n",
    "epsilons = [0.1, 0.2]\n",
    "result = {}\n",
    "\n",
    "for epsilon in epsilons:\n",
    "    Q = init_q_table(n_state, n_actions)\n",
    "    total_rewards = []\n",
    "    avg_rewards = []\n",
    "    for episode in range(episodes):\n",
    "        #print(episode)\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "    \n",
    "        done = False\n",
    "        while not done:\n",
    "            state_idx = env.get_state_nos()\n",
    "            action = select_action(Q, state_idx, epsilon)\n",
    "            next_state, reward, done = env.step(action)\n",
    "            next_state_idx = env.get_state_nos()\n",
    "            episode_reward += reward\n",
    "            if not done:\n",
    "                q_update(Q, state_idx, action, reward, next_state_idx, alpha, gamma)\n",
    "            elif next_state == (3,3):\n",
    "                print(f\"target reached on episode : {episode}\")    \n",
    "            \n",
    "        total_rewards.append(episode_reward)   \n",
    "    \n",
    "        if episode >= 49:\n",
    "            average_reward = np.mean(total_rewards[-50:])\n",
    "            avg_rewards.append(average_reward)\n",
    "            \n",
    "    result[epsilon] = avg_rewards        \n",
    "\n",
    "%matplotlib inline\n",
    "for epsilon, rewards in result.items():\n",
    "    plt.plot(rewards, label=f\"ε = {epsilon}\") \n",
    "\n",
    "plt.title(\"Moving Average Reward over Episodes\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Average Reward (last 50 episodes)\")\n",
    "plt.legend()       \n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb54dbb-c702-4aea-ada9-90b788f9e924",
   "metadata": {},
   "source": [
    "Task 4: Deep Q-Network with Target Copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f93a692f-eda0-4a2e-8db1-42bb18a5b02c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class DQNNetwork(nn.Module):\n",
    "    def __init__(self, obs_dim, n_actions):\n",
    "        super(DQNNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.output = nn.Linear(128, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.output(x)\n",
    "\n",
    "# Defining policy and Target classes\n",
    "class DQNPolicy(DQNNetwork):\n",
    "    pass\n",
    "\n",
    "class DQNTarget(DQNNetwork):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37047bfe-c8e0-434b-a606-b6ad444371f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_target(policy_net, target_net):\n",
    "    target_net.load_state_dict(policy_net.state_dict())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1cec1f-3c12-433f-93cc-38ca174a4a31",
   "metadata": {},
   "source": [
    "Task 5: Full DQN Training Loop on CartPole-v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cb62f2f-03b0-4ce6-84a3-a6c7ace27c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Environment setup\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "\n",
    "# Initialising networks\n",
    "policy_net = DQN(state_dim, action_dim)\n",
    "target_net = DQN(state_dim, action_dim)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "target_net.eval()\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(policy_net.parameters(), lr=1e-3)\n",
    "loss_fn = nn.MSELoss()\n",
    "\n",
    "# Hyperparameters\n",
    "num_episodes = 500\n",
    "gamma = 0.99\n",
    "epsilon = 1.0\n",
    "epsilon_min = 0.01\n",
    "epsilon_decay = 1 / 500\n",
    "target_update_freq = 10\n",
    "\n",
    "# Target network update function\n",
    "def update_target(policy_net, target_net):\n",
    "    target_net.load_state_dict(policy_net.state_dict())\n",
    "\n",
    "# ε-greedy action selection\n",
    "def select_action(state, epsilon):\n",
    "    if random.random() < epsilon:\n",
    "        return random.randint(0, action_dim - 1)\n",
    "    with torch.no_grad():\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = policy_net(state_tensor)\n",
    "        return torch.argmax(q_values).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f68bd7fb-e9da-4e3c-8025-b867f3fdd0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "rewards = []\n",
    "\n",
    "for episode in range(num_episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    total_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        action = select_action(state, epsilon)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "\n",
    "        with torch.no_grad():\n",
    "            next_state_tensor = torch.FloatTensor(next_state).unsqueeze(0)\n",
    "            max_next_q = target_net(next_state_tensor).max(1)[0].item()\n",
    "            target = reward + (0 if done else gamma * max_next_q)\n",
    "\n",
    "        state_tensor = torch.FloatTensor(state).unsqueeze(0)\n",
    "        q_values = policy_net(state_tensor)\n",
    "        q_value = q_values[0, action]\n",
    "\n",
    "        loss = loss_fn(q_value, torch.tensor(target))\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        state = next_state\n",
    "        epsilon = max(epsilon_min, epsilon - epsilon_decay)\n",
    "\n",
    "    rewards.append(total_reward)\n",
    "\n",
    "    if (episode + 1) % target_update_freq == 0:\n",
    "        update_target(policy_net, target_net)\n",
    "\n",
    "    if (episode + 1) % 10 == 0:\n",
    "        avg = np.mean(rewards[-10:])\n",
    "        print(f\"Episode {episode+1}, Avg Reward: {avg:.2f}, Epsilon: {epsilon:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "054fe8ff-aa43-4edc-97b4-e8c89a6352bb",
   "metadata": {},
   "source": [
    "I will be really honest while giving explanation to the solution to the fifth question.\n",
    "In the week 1 resourses the DQN was implemented using tensorflow and keras and use of Pytorch to implment DQN was not there so, while doing this task I had taken help from youtube videos and chatgpt . Like I just asked gpt to give me a basic structure of code to implement DQN using Pytorch and I asked for the whole explanation of the code. and from that help I had written the code .\n",
    "In the rest all tasks and I had submitted my own written code."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
